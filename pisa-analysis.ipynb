{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7415861,"sourceType":"datasetVersion","datasetId":4287388}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-01T15:49:36.791023Z","iopub.execute_input":"2024-02-01T15:49:36.791500Z","iopub.status.idle":"2024-02-01T15:49:36.803978Z","shell.execute_reply.started":"2024-02-01T15:49:36.791459Z","shell.execute_reply":"2024-02-01T15:49:36.802255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Done together with @matrodriguez","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport sklearn.preprocessing as skp\nimport sklearn.decomposition as skd\nimport sklearn.manifold as skm\nimport sklearn.cluster as skc\nimport scipy.cluster.hierarchy as scich \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.model_selection as skms \nimport sklearn.preprocessing as skpp \nimport sklearn.metrics as skm \nimport sklearn.pipeline as skpl \nimport sklearn.tree as skt \nimport sklearn.neighbors as skn \nimport sklearn.svm as skv \nimport sklearn.ensemble as ske\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nfrom math import isnan\nfrom sklearn.cluster import KMeans\nimport pyclustering\nfrom sklearn_extra.cluster import KMedoids\nfrom sklearn.datasets import make_blobs\nfrom mpl_toolkits import mplot3d\nimport plotly.express as px\nimport kmedoids\nimport econml\nfrom sklearn.linear_model import TheilSenRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:36.807287Z","iopub.execute_input":"2024-02-01T15:49:36.807870Z","iopub.status.idle":"2024-02-01T15:49:36.825914Z","shell.execute_reply.started":"2024-02-01T15:49:36.807827Z","shell.execute_reply":"2024-02-01T15:49:36.824269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = pd.read_csv(\"/kaggle/input/pisa-results-2000-2022-economics-and-education/economics_and_education_dataset_CSV.csv\")\npd.set_option('display.max_rows',None)\npd.set_option('display.max_columns',None)\n#p.columns.to_numpy()\np = p.drop('index_code', axis = 1)\np = p.drop('time', axis = 1)\np = p.drop('sex', axis = 1)\np","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:36.828705Z","iopub.execute_input":"2024-02-01T15:49:36.829297Z","iopub.status.idle":"2024-02-01T15:49:37.528254Z","shell.execute_reply.started":"2024-02-01T15:49:36.829251Z","shell.execute_reply":"2024-02-01T15:49:37.527347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p.columns.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:37.530644Z","iopub.execute_input":"2024-02-01T15:49:37.531582Z","iopub.status.idle":"2024-02-01T15:49:37.539505Z","shell.execute_reply.started":"2024-02-01T15:49:37.531539Z","shell.execute_reply":"2024-02-01T15:49:37.538365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dictionary:\ncountry_prefix = p.country.unique()\ncountries_dict = {elem : pd.DataFrame() for elem in country_prefix}\nfor key in countries_dict.keys():\n    countries_dict[key] = p[:][p.country == key]\n    \nfor country in country_prefix:\n    for column in countries_dict[country].select_dtypes(include='number'):\n        median_country = countries_dict[country][column].median()\n        countries_dict[country][column] = countries_dict[country][column].fillna(median_country)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:37.541195Z","iopub.execute_input":"2024-02-01T15:49:37.541584Z","iopub.status.idle":"2024-02-01T15:49:37.859622Z","shell.execute_reply.started":"2024-02-01T15:49:37.541552Z","shell.execute_reply":"2024-02-01T15:49:37.858543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_joineds = pd.concat(countries_dict.values())\ndf_joineds.dropna(how = 'any', inplace = True)\ndf_joineds.drop('country',axis = 1, inplace = True)\ndf_joineds","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:37.861147Z","iopub.execute_input":"2024-02-01T15:49:37.861645Z","iopub.status.idle":"2024-02-01T15:49:38.477418Z","shell.execute_reply.started":"2024-02-01T15:49:37.861584Z","shell.execute_reply":"2024-02-01T15:49:38.475924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df_joineds[['rating']]\nx = df_joineds[['expenditure_on _education_pct_gdp', 'mortality_rate_infant',\n       'gini_index', 'inflation_consumer_prices',\n       'intentional_homicides', 'unemployment',\n       'gross_fixed_capital_formation', 'population_density',\n       'suicide_mortality_rate', 'tax_revenue',\n       'taxes_on_income_profits_capital',\n       'alcohol_consumption_per_capita',\n       'government_health_expenditure_pct_gdp',\n       'urban_population_pct_total']]\n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:38.479184Z","iopub.execute_input":"2024-02-01T15:49:38.479587Z","iopub.status.idle":"2024-02-01T15:49:38.490100Z","shell.execute_reply.started":"2024-02-01T15:49:38.479555Z","shell.execute_reply":"2024-02-01T15:49:38.488471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df_joineds.corr()\ncorr.style.background_gradient(cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:38.492283Z","iopub.execute_input":"2024-02-01T15:49:38.492735Z","iopub.status.idle":"2024-02-01T15:49:38.552900Z","shell.execute_reply.started":"2024-02-01T15:49:38.492703Z","shell.execute_reply":"2024-02-01T15:49:38.551366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Q1 = df_joineds['rating'].quantile(0.25)\nQ3 = df_joineds['rating'].quantile(0.75)\nIQR = Q3 - Q1\n((df_joineds['rating'] < (Q1 - 1.5 * IQR)) | (df_joineds['rating'] > (Q3 + 1.5 * IQR))).sum()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:38.554881Z","iopub.execute_input":"2024-02-01T15:49:38.555304Z","iopub.status.idle":"2024-02-01T15:49:38.570631Z","shell.execute_reply.started":"2024-02-01T15:49:38.555264Z","shell.execute_reply":"2024-02-01T15:49:38.569095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.columns.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:38.577055Z","iopub.execute_input":"2024-02-01T15:49:38.577528Z","iopub.status.idle":"2024-02-01T15:49:38.587270Z","shell.execute_reply.started":"2024-02-01T15:49:38.577497Z","shell.execute_reply":"2024-02-01T15:49:38.585826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# huber reg\nimport statsmodels.api as sm\nimport statsmodels as smd\nimport statsmodels.formula.api as smf","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:38.589102Z","iopub.execute_input":"2024-02-01T15:49:38.589512Z","iopub.status.idle":"2024-02-01T15:49:38.596007Z","shell.execute_reply.started":"2024-02-01T15:49:38.589481Z","shell.execute_reply":"2024-02-01T15:49:38.594357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"huber_t = sm.RLM(endog = y, \n                   exog= x, M=sm.robust.norms.HuberT())\nhub_results = huber_t.fit()\nprint(hub_results.params)\nprint(\n    hub_results.summary(\n        yname=\"y\", xname=[\"var_%d\" % i for i in range(len(hub_results.params))]\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:38.597828Z","iopub.execute_input":"2024-02-01T15:49:38.598260Z","iopub.status.idle":"2024-02-01T15:49:38.641058Z","shell.execute_reply.started":"2024-02-01T15:49:38.598228Z","shell.execute_reply":"2024-02-01T15:49:38.640077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#r2:\nr2_wls = sm.WLS(huber_t.endog, huber_t.exog, weights=huber_t.fit().weights).fit().rsquared\nr2_wls","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:38.642471Z","iopub.execute_input":"2024-02-01T15:49:38.642870Z","iopub.status.idle":"2024-02-01T15:49:38.670556Z","shell.execute_reply.started":"2024-02-01T15:49:38.642837Z","shell.execute_reply":"2024-02-01T15:49:38.669370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Quantile Regression\nquanti = smf.quantreg('y ~ x' ,df_joineds).fit(q=0.7)\n \n# view model summary\nprint(quanti.summary())","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:38.672373Z","iopub.execute_input":"2024-02-01T15:49:38.673281Z","iopub.status.idle":"2024-02-01T15:49:38.977931Z","shell.execute_reply.started":"2024-02-01T15:49:38.673248Z","shell.execute_reply":"2024-02-01T15:49:38.976502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quasi-experiment:\nscaler = skp.StandardScaler( )\np_norm = pd.DataFrame( scaler.fit_transform( df_joineds.values ), index = df_joineds.index, columns = df_joineds.columns)\np_norm = p_norm.dropna(how='any')\np_norm","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:38.979337Z","iopub.execute_input":"2024-02-01T15:49:38.979702Z","iopub.status.idle":"2024-02-01T15:49:39.441239Z","shell.execute_reply.started":"2024-02-01T15:49:38.979673Z","shell.execute_reply":"2024-02-01T15:49:39.440056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = skd.PCA(svd_solver='randomized', random_state=42)\n# fiting PCA on the dataset\npca.fit(p_norm)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:39.443287Z","iopub.execute_input":"2024-02-01T15:49:39.443763Z","iopub.status.idle":"2024-02-01T15:49:39.486059Z","shell.execute_reply.started":"2024-02-01T15:49:39.443724Z","shell.execute_reply":"2024-02-01T15:49:39.484111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca.explained_variance_ratio_","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:39.488599Z","iopub.execute_input":"2024-02-01T15:49:39.491244Z","iopub.status.idle":"2024-02-01T15:49:39.507219Z","shell.execute_reply.started":"2024-02-01T15:49:39.491184Z","shell.execute_reply":"2024-02-01T15:49:39.505412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:39.509382Z","iopub.execute_input":"2024-02-01T15:49:39.511277Z","iopub.status.idle":"2024-02-01T15:49:39.826832Z","shell.execute_reply.started":"2024-02-01T15:49:39.511219Z","shell.execute_reply":"2024-02-01T15:49:39.825657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_5c_obj = skd.PCA( n_components = 6) \npca_d = pca_5c_obj.fit_transform( p_norm.values ) \n\npca_5c_obj.explained_variance_ratio_\nsum( pca_5c_obj.explained_variance_ratio_ )","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:39.828325Z","iopub.execute_input":"2024-02-01T15:49:39.829443Z","iopub.status.idle":"2024-02-01T15:49:39.852124Z","shell.execute_reply.started":"2024-02-01T15:49:39.829404Z","shell.execute_reply":"2024-02-01T15:49:39.850406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import IncrementalPCA\npca_f_d = IncrementalPCA(n_components=6)\ndf_pca_ = pca_f_d.fit_transform(p_norm)\ndf_pca_.shape\npc_d = np.transpose(df_pca_)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:39.854657Z","iopub.execute_input":"2024-02-01T15:49:39.855733Z","iopub.status.idle":"2024-02-01T15:49:39.880046Z","shell.execute_reply.started":"2024-02-01T15:49:39.855672Z","shell.execute_reply":"2024-02-01T15:49:39.878443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrmat = np.corrcoef(pc_d)\nplt.figure(figsize = (20,10))\nsns.heatmap(corrmat,annot = True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:39.882312Z","iopub.execute_input":"2024-02-01T15:49:39.883259Z","iopub.status.idle":"2024-02-01T15:49:40.367743Z","shell.execute_reply.started":"2024-02-01T15:49:39.883197Z","shell.execute_reply":"2024-02-01T15:49:40.366498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pcr_df = pd.DataFrame({'PC1':pc_d[0],'PC2':pc_d[1],'PC3':pc_d[2],'PC4':pc_d[3],'PC5':pc_d[4],'PC6':pc_d[5]})\npcr_df.shape \npcr_df","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:40.369173Z","iopub.execute_input":"2024-02-01T15:49:40.369526Z","iopub.status.idle":"2024-02-01T15:49:40.542140Z","shell.execute_reply.started":"2024-02-01T15:49:40.369497Z","shell.execute_reply":"2024-02-01T15:49:40.540860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clusterization:\nssd = []\nfor num_clusters in list(range(1,10)):\n    model_clus = KMedoids(n_clusters = num_clusters, max_iter=50)\n    model_clus.fit(pcr_df)\n    ssd.append(model_clus.inertia_)\n\nplt.plot(ssd)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:40.543661Z","iopub.execute_input":"2024-02-01T15:49:40.544122Z","iopub.status.idle":"2024-02-01T15:49:41.005245Z","shell.execute_reply.started":"2024-02-01T15:49:40.544083Z","shell.execute_reply":"2024-02-01T15:49:41.004016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_m = KMedoids(n_clusters=8, random_state=0)\nk_m.fit(pcr_df)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:41.006741Z","iopub.execute_input":"2024-02-01T15:49:41.007205Z","iopub.status.idle":"2024-02-01T15:49:41.039882Z","shell.execute_reply.started":"2024-02-01T15:49:41.007163Z","shell.execute_reply":"2024-02-01T15:49:41.038299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_m.labels_","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:41.042671Z","iopub.execute_input":"2024-02-01T15:49:41.043891Z","iopub.status.idle":"2024-02-01T15:49:41.068595Z","shell.execute_reply.started":"2024-02-01T15:49:41.043834Z","shell.execute_reply":"2024-02-01T15:49:41.067076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_ml = pd.DataFrame(k_m.labels_)\nk_ml.columns = ['Cluster_md']\nk_ml","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:41.071294Z","iopub.execute_input":"2024-02-01T15:49:41.072555Z","iopub.status.idle":"2024-02-01T15:49:41.144361Z","shell.execute_reply.started":"2024-02-01T15:49:41.072496Z","shell.execute_reply":"2024-02-01T15:49:41.143505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_joineds.reset_index(drop=True, inplace=True)\nk_m = pd.concat([k_ml,df_joineds],axis = 1)\nk_m","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:41.145695Z","iopub.execute_input":"2024-02-01T15:49:41.146048Z","iopub.status.idle":"2024-02-01T15:49:41.764050Z","shell.execute_reply.started":"2024-02-01T15:49:41.146020Z","shell.execute_reply":"2024-02-01T15:49:41.762635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x= k_m['Cluster_md'],y='rating',data=k_m)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:41.770641Z","iopub.execute_input":"2024-02-01T15:49:41.771086Z","iopub.status.idle":"2024-02-01T15:49:42.040022Z","shell.execute_reply.started":"2024-02-01T15:49:41.771051Z","shell.execute_reply":"2024-02-01T15:49:42.038427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c0 =k_m.query('Cluster_md == 0')\nc1 =k_m.query('Cluster_md == 1')\nc2 =k_m.query('Cluster_md == 2')\nc3 =k_m.query('Cluster_md == 3')\nc4 =k_m.query('Cluster_md == 4')\nc5 =k_m.query('Cluster_md == 5')\nc6 =k_m.query('Cluster_md == 6')\nc7 =k_m.query('Cluster_md == 7')","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:42.041917Z","iopub.execute_input":"2024-02-01T15:49:42.043410Z","iopub.status.idle":"2024-02-01T15:49:42.083527Z","shell.execute_reply.started":"2024-02-01T15:49:42.043280Z","shell.execute_reply":"2024-02-01T15:49:42.082580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dd= pd.concat([c0,c1],ignore_index=True)\ndd.columns.to_numpy() ","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:42.084801Z","iopub.execute_input":"2024-02-01T15:49:42.086536Z","iopub.status.idle":"2024-02-01T15:49:42.098875Z","shell.execute_reply.started":"2024-02-01T15:49:42.086475Z","shell.execute_reply":"2024-02-01T15:49:42.097290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from scipy.stats import ttest_ind\n#_, p = ttest_ind(dd.expenditure_on _education_pct_gdp, df_joineds.expenditure_on _education_pct_gdp)\n#print(f'p={p:.3f}')\n\n# interpret\n#alpha = 0.05  # significance level\n#if p > alpha:\n  #  print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\n#else:\n #   print('different distributions/different group mean (reject H0)')\n\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.rating, k_m.rating)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\n    \n\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.mortality_rate_infant, k_m.mortality_rate_infant)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\n    \nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.gini_index, k_m.gini_index)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.gdp_per_capita_ppp, k_m.gdp_per_capita_ppp)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\n\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.inflation_consumer_prices, k_m.inflation_consumer_prices)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.intentional_homicides, k_m.intentional_homicides)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\n\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.unemployment, k_m.unemployment)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\n\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.gross_fixed_capital_formation, k_m.gross_fixed_capital_formation)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\n\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.population_density, k_m.population_density)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\n\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.suicide_mortality_rate, k_m.suicide_mortality_rate)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\n\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.alcohol_consumption_per_capita, k_m.alcohol_consumption_per_capita)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\n\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.taxes_on_income_profits_capital, k_m.taxes_on_income_profits_capital)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\n\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.government_health_expenditure_pct_gdp, k_m.government_health_expenditure_pct_gdp)\nprint(f'p={p:.3f}')\n\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')\n\nfrom scipy.stats import ttest_ind\n_, p = ttest_ind(dd.urban_population_pct_total, k_m.urban_population_pct_total)\nprint(f'p={p:.3f}')\n# interpret\nalpha = 0.05  # significance level\nif p > alpha:\n    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\nelse:\n    print('different distributions/different group mean (reject H0)')","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:42.100766Z","iopub.execute_input":"2024-02-01T15:49:42.101168Z","iopub.status.idle":"2024-02-01T15:49:42.411080Z","shell.execute_reply.started":"2024-02-01T15:49:42.101124Z","shell.execute_reply":"2024-02-01T15:49:42.409679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Logit:\nx_ = dd[['expenditure_on _education_pct_gdp',\n       'gini_index', 'gdp_per_capita_ppp',\n       'inflation_consumer_prices', \n       'unemployment', 'gross_fixed_capital_formation',\n       'suicide_mortality_rate', 'tax_revenue',\n       'taxes_on_income_profits_capital',\n       'alcohol_consumption_per_capita',\n      'urban_population_pct_total' ]]\n\ny_ = dd[['Cluster_md']]","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:42.412566Z","iopub.execute_input":"2024-02-01T15:49:42.412964Z","iopub.status.idle":"2024-02-01T15:49:42.421553Z","shell.execute_reply.started":"2024-02-01T15:49:42.412933Z","shell.execute_reply":"2024-02-01T15:49:42.420129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l =   'urban_population_pct_total'\nQ1 = dd[l].quantile(0.25)\nQ3 = dd[l].quantile(0.75)\nIQR = Q3 - Q1\n((dd[l] < (Q1 - 1.5 * IQR)) | (dd[l] > (Q3 + 1.5 * IQR))).sum()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:42.423038Z","iopub.execute_input":"2024-02-01T15:49:42.423436Z","iopub.status.idle":"2024-02-01T15:49:42.444216Z","shell.execute_reply.started":"2024-02-01T15:49:42.423408Z","shell.execute_reply":"2024-02-01T15:49:42.442904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nimport statsmodels.discrete.discrete_model as sm\nmodel= sm.Logit(y_,x_)\nco =model.fit().params\ncoe = pd.DataFrame(co)\ncoe.columns = ['coeff']\nexp_coeff =   np.exp(coe)/(np.exp(coe) + 1) \nexp_coeff = pd.DataFrame(exp_coeff)\nexp_coeff.columns = ['exp_coeff']\nexp_coeff","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:42.445581Z","iopub.execute_input":"2024-02-01T15:49:42.446112Z","iopub.status.idle":"2024-02-01T15:49:42.477926Z","shell.execute_reply.started":"2024-02-01T15:49:42.445929Z","shell.execute_reply":"2024-02-01T15:49:42.476787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = model.fit()\nresults_summary = results.summary()\nresults_summary","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:49:42.479792Z","iopub.execute_input":"2024-02-01T15:49:42.480826Z","iopub.status.idle":"2024-02-01T15:49:42.542036Z","shell.execute_reply.started":"2024-02-01T15:49:42.480786Z","shell.execute_reply":"2024-02-01T15:49:42.540801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_as_html = results_summary.tables[1].as_html()\ndf_results = pd.read_html(results_as_html, header=0, index_col=0)[0]\ndf_results['exp_coeff'] = exp_coeff\ndf_results","metadata":{"execution":{"iopub.status.busy":"2024-02-01T15:57:02.432169Z","iopub.execute_input":"2024-02-01T15:57:02.432662Z","iopub.status.idle":"2024-02-01T15:57:02.534046Z","shell.execute_reply.started":"2024-02-01T15:57:02.432627Z","shell.execute_reply":"2024-02-01T15:57:02.532905Z"},"trusted":true},"execution_count":null,"outputs":[]}]}